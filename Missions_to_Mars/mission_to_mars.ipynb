{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import pandas as pd\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Browser Funcion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_browser():\n",
    "    # @NOTE: Replace the path with your actual path to the chromedriver\n",
    "    executable_path = {\"executable_path\": \"/usr/local/bin/chromedriver\"}\n",
    "    return Browser(\"chrome\", **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Initialize Browser Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run init_browser function and open it\n",
    "# browser = init_browser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NASA Mars News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run init_browser function and open it\n",
    "browser = init_browser()\n",
    "# Visit https://mars.nasa.gov/news/\n",
    "url = \"https://mars.nasa.gov/news/\"\n",
    "browser.visit(url)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# Scrape page into Soup\n",
    "html = browser.html\n",
    "soup = bs(html, \"html.parser\")\n",
    "# Get Title and Paragrapht Text\n",
    "news_title = soup.find('div', class_=\"content_title\").text\n",
    "news_p = soup.find('div', class_=\"article_teaser_body\").text\n",
    "\n",
    "browser.quit()\n",
    "print(news_title + \"\\n\" +  news_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JPL Mars Space Images - Featured Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run init_browser function and open it\n",
    "browser = init_browser()\n",
    "# Visit https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\n",
    "url = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "browser.visit(url)\n",
    "\n",
    "# Scrape page into Soup\n",
    "html = browser.html\n",
    "soup = bs(html, \"html.parser\")\n",
    "time.sleep(1)\n",
    "\n",
    "# Click in one of the images\n",
    "browser.click_link_by_partial_text('Curiosity')\n",
    "time.sleep(5)\n",
    "\n",
    "# Click in the button for more information\n",
    "browser.click_link_by_partial_text('more info')\n",
    "time.sleep(1)\n",
    "\n",
    "# Get the new url after clicking \"more info\" and visiting it\n",
    "url = browser.url\n",
    "browser.visit(url)\n",
    "time.sleep(1)\n",
    "\n",
    "# Scrape clicked page into Soup\n",
    "html = browser.html\n",
    "soup = bs(html, \"html.parser\")\n",
    "\n",
    "# Get the base url to be used with relative paths\n",
    "parsed = urlparse(url)\n",
    "base_url = parsed.scheme +\"://\"+ parsed.netloc\n",
    "\n",
    "# Get the high resolution image from the accesed page\n",
    "featured_image_url = base_url +  soup.find('img', class_=\"main_image\")[\"src\"]\n",
    "\n",
    "browser.quit()\n",
    "print(featured_image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run init_browser function and open it\n",
    "browser = init_browser()\n",
    "# Visit https://twitter.com/marswxreport?lang=en\n",
    "url = \"https://twitter.com/marswxreport?lang=en\"\n",
    "browser.visit(url)\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "# Scrape page into Soup\n",
    "html = browser.html\n",
    "soup = bs(html, \"html.parser\")\n",
    "\n",
    "# Get the image URL\n",
    "\n",
    "# Get all the text\n",
    "mars_weather_p_a = soup.find(\"div\", class_=\"js-tweet-text-container\").text\n",
    "\n",
    "# Get the text in the last part\n",
    "mars_weather_a = soup.find(\"div\", class_=\"js-tweet-text-container\").findChildren()[1].text\n",
    "\n",
    "# Eliminate the last part of the text\n",
    "mars_weather = mars_weather_p_a.replace(mars_weather_a,'')\n",
    "\n",
    "browser.quit()\n",
    "print(mars_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run init_browser function and open it\n",
    "browser = init_browser()\n",
    "# Visit https://space-facts.com/mars/\n",
    "url = \"https://space-facts.com/mars/\"\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# Scrape with Paandas\n",
    "tables_df = pd.read_html(url)\n",
    "\n",
    "# Get first table with the facts\n",
    "mars_facts_df = tables_df[1]\n",
    "\n",
    "# Rename column of the dataframe\n",
    "mars_facts_df = mars_facts_df.rename(columns={0: \"Description\", 1: \"Value\"})\n",
    "\n",
    "\n",
    "# Use the titles as index\n",
    "mars_facts_df.set_index(\"Description\", inplace=True)\n",
    "\n",
    "# Convert the dataframe to html\n",
    "mars_facts_html = mars_facts_df.to_html()\n",
    "\n",
    "browser.quit()\n",
    "print (mars_facts_html)\n",
    "print (mars_facts_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run init_browser function and open it\n",
    "browser = init_browser()\n",
    "\n",
    "# Visit https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\n",
    "url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "\n",
    "# Get the base url to be used with relative paths\n",
    "parsed = urlparse(url)\n",
    "base_url = parsed.scheme +\"://\"+ parsed.netloc\n",
    "\n",
    "browser.visit(url)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# Scrape page into Soup\n",
    "html = browser.html\n",
    "soup = bs(html, \"html.parser\")\n",
    "\n",
    "# Get the image URL\n",
    "\n",
    "results = soup.find(\"div\", class_=\"collapsible results\")\n",
    "\n",
    "# Initialize the Dictionary\n",
    "hemisphere_image_urls = []\n",
    "# Loop through returned results\n",
    "for result in results:\n",
    "    # Error handling\n",
    "    try:\n",
    "        # Identify and return title of the image\n",
    "        title = result.find(\"div\").h3.text\n",
    "        \n",
    "        # Identify and return link to the high resolution image\n",
    "        link_parent = base_url + result.a['href']\n",
    "\n",
    "        # Use the link to the page to get the high resolution Page\n",
    "        link_son = link_parent\n",
    "        browser.visit(link_son)\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Scrape the socond page into Soup\n",
    "        html = browser.html\n",
    "        soup = bs(html, \"html.parser\")\n",
    "\n",
    "        # Get the image URL of the high resolution URL\n",
    "        # For getting the Original .tif\n",
    "        link_son = soup.find(\"a\", text=\"Original\")['href']\n",
    "        \n",
    "        # For getting the Sample .jpg\n",
    "        #link_son = soup.find(\"a\", text=\"Sample\")['href']\n",
    "\n",
    "        # Print results only if title, price, and link are available\n",
    "        if (title and link_parent):\n",
    "            #print('-------------')\n",
    "            #print(title)\n",
    "            #print(link_parent)\n",
    "            #print(link_son)\n",
    "\n",
    "            # Create Dictionary\n",
    "            url_dict = {}\n",
    "            url_dict[\"title\"] = title\n",
    "            url_dict[\"img_url\"] = link_son\n",
    "            hemisphere_image_urls.append(url_dict)\n",
    "\n",
    "    except AttributeError as e:\n",
    "            #print(e)\n",
    "            pass\n",
    "\n",
    "browser.quit()\n",
    "print (hemisphere_image_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Result Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data in a dictionary\n",
    "scraping_results = {\n",
    "        \"news_title\": news_title,\n",
    "        \"news_p\": news_p,\n",
    "        \"featured_image_url\": featured_image_url,\n",
    "        \"mars_weather\": mars_weather,\n",
    "        \"mars_facts_html\": mars_facts_html,\n",
    "        \"hemisphere_image_urls\": hemisphere_image_urls\n",
    "    }\n",
    "scraping_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
